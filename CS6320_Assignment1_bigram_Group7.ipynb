{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------\n",
        "# CS6320 - NLP - Assignment 1 - Bigram Model\n",
        "# -------------------------------------\n",
        "\n",
        "print(\"\\n-------------------------\\nGroup 7 - NLP Assignment 1\")\n",
        "print(\"Bhanu Maneesh Reddy Mannem (BXM220055)\")\n",
        "print(\"Snehal Kumar Ketala (SXK220463)\")\n",
        "print(\"Lalithya Mada (LXM230002)\")\n",
        "print(\"-------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFIrrkf0bjNF",
        "outputId": "dd9e4987-e541-43ae-970f-912b8c9e883c"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-------------------------\n",
            "Group 7 - NLP Assignment 1\n",
            "Bhanu Maneesh Reddy Mannem (BXM220055)\n",
            "Snehal Kumar Ketala (SXK220463)\n",
            "Lalithya Mada (LXM230002)\n",
            "-------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "import math\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "J--5Hw7OZdXB"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Data Download and Preprocessing**"
      ],
      "metadata": {
        "id": "l_Db3Y-WcDQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_dataset():\n",
        "    train_url = \"https://raw.githubusercontent.com/maneeshmbr/unigram-and-bigram-models---NLP/master/A1_DATASET/A1_DATASET/train.txt\"\n",
        "    val_url = \"https://raw.githubusercontent.com/maneeshmbr/unigram-and-bigram-models---NLP/master/A1_DATASET/A1_DATASET/val.txt\"\n",
        "\n",
        "    train_data = requests.get(train_url).text\n",
        "    with open('training_data.txt', 'w') as train_file:\n",
        "        train_file.write(train_data)\n",
        "\n",
        "    val_data = requests.get(val_url).text\n",
        "    with open('validation_data.txt', 'w') as val_file:\n",
        "        val_file.write(val_data)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    clean_text = re.sub(r'\\W', ' ', text).lower()\n",
        "    tokens = clean_text.split()\n",
        "    bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens) - 1)]\n",
        "    return tokens, bigrams\n",
        "\n",
        "def load_and_preprocess(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    tokens, bigrams = preprocess_text(text)\n",
        "    return tokens, bigrams\n",
        "\n",
        "download_dataset()\n",
        "train_tokens, train_bigrams = load_and_preprocess('training_data.txt')\n",
        "val_tokens, val_bigrams = load_and_preprocess('validation_data.txt')\n"
      ],
      "metadata": {
        "id": "BLBntsJPZhwN"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unsmoothed Bigram Calculation and its probability**"
      ],
      "metadata": {
        "id": "QEVY77GTcNpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bigram_probabilities(tokens, bigrams):\n",
        "    unigram_counts = defaultdict(int)\n",
        "    bigram_counts = defaultdict(int)\n",
        "\n",
        "    for token in tokens:\n",
        "        unigram_counts[token] += 1\n",
        "\n",
        "    for bigram in bigrams:\n",
        "        bigram_counts[bigram] += 1\n",
        "\n",
        "    bigram_probs = {bigram: bigram_counts[bigram] / unigram_counts[bigram[0]] for bigram in bigram_counts}\n",
        "    bigram_log_probs = {bigram: math.log(bigram_probs[bigram]) for bigram in bigram_probs}\n",
        "\n",
        "    return bigram_probs, bigram_log_probs, unigram_counts, bigram_counts\n",
        "\n",
        "bigram_probs, bigram_log_probs, unigram_counts, bigram_counts = calculate_bigram_probabilities(train_tokens, train_bigrams)\n"
      ],
      "metadata": {
        "id": "HQfB7slwZkQj"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perplexity Calculation**"
      ],
      "metadata": {
        "id": "mdc_I41lc-5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(bigram_probs, val_bigrams, log_prob_dict):\n",
        "    val_bigram_counts = defaultdict(int)\n",
        "    token_count = 0\n",
        "    for bigram in val_bigrams:\n",
        "        val_bigram_counts[bigram] += 1\n",
        "        token_count += 1\n",
        "\n",
        "    log_prob_sum = 0\n",
        "    for bigram in val_bigram_counts:\n",
        "        log_prob_sum += (-1) * log_prob_dict.get(bigram, log_prob_dict.get(('unk', 'unk'), 0)) * val_bigram_counts[bigram]\n",
        "\n",
        "    return math.exp(log_prob_sum / token_count)\n"
      ],
      "metadata": {
        "id": "3NwGE0uXZnr9"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Smoothing (Laplace and Add-k)**"
      ],
      "metadata": {
        "id": "U_T_WOyfdDlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def laplace_smoothing(bigram_counts, unigram_counts, vocab_size):\n",
        "    laplace_probs = {}\n",
        "    laplace_log_probs = {}\n",
        "\n",
        "    for bigram in bigram_counts:\n",
        "        laplace_probs[bigram] = (bigram_counts[bigram] + 1) / (unigram_counts[bigram[0]] + vocab_size)\n",
        "        laplace_log_probs[bigram] = math.log(laplace_probs[bigram])\n",
        "\n",
        "    return laplace_probs, laplace_log_probs\n",
        "\n",
        "def add_k_smoothing(bigram_counts, unigram_counts, k, vocab_size):\n",
        "    add_k_probs = {}\n",
        "    add_k_log_probs = {}\n",
        "\n",
        "    for bigram in bigram_counts:\n",
        "        add_k_probs[bigram] = (bigram_counts[bigram] + k) / (unigram_counts[bigram[0]] + k * vocab_size)\n",
        "        add_k_log_probs[bigram] = math.log(add_k_probs[bigram])\n",
        "\n",
        "    return add_k_probs, add_k_log_probs\n"
      ],
      "metadata": {
        "id": "Ol8YFkpWZqGC"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling Unknown Words (Unk) and Rare Words**"
      ],
      "metadata": {
        "id": "OCECLOs6dI_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_unknown_with_zero_frequency(tokens):\n",
        "    \"\"\"\n",
        "    Replace words that occur only once with 'unk' and keeps 'unk' with zero frequency.\n",
        "    \"\"\"\n",
        "    unigram_counts = defaultdict(int)\n",
        "    for token in tokens:\n",
        "        unigram_counts[token] += 1\n",
        "\n",
        "    # Add 'unk' with zero frequency\n",
        "    unigram_counts['unk'] = 0\n",
        "\n",
        "    tokens_unk = ['unk' if unigram_counts[token] == 1 else token for token in tokens]\n",
        "    return tokens_unk, unigram_counts\n",
        "\n",
        "def handle_rare_words(tokens, rare_threshold=2):\n",
        "    \"\"\"\n",
        "    Replace words that occur less than or equal to the threshold with 'unk'.\n",
        "    \"\"\"\n",
        "    word_counts = defaultdict(int)\n",
        "    for token in tokens:\n",
        "        word_counts[token] += 1\n",
        "\n",
        "    tokens_rare_unk = ['unk' if word_counts[token] <= rare_threshold else token for token in tokens]\n",
        "    return tokens_rare_unk\n",
        "\n",
        "# Handle unknown words with zero frequency\n",
        "train_tokens_unk, unigram_counts_unk = handle_unknown_with_zero_frequency(train_tokens)\n",
        "# Handle rare words with a threshold of 2\n",
        "train_tokens_rare_unk = handle_rare_words(train_tokens)\n"
      ],
      "metadata": {
        "id": "rxYjmDBpZtVO"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perplexity Calculations for All Scenarios**"
      ],
      "metadata": {
        "id": "fhk5ZIjkdOkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_all_perplexities():\n",
        "    results = {}\n",
        "    vocab_size = len(unigram_counts)\n",
        "\n",
        "    # 1. Perplexity with No Smoothing\n",
        "    perplexity_no_smoothing = calculate_perplexity(bigram_probs, val_bigrams, bigram_log_probs)\n",
        "    results['Perplexity - No Smoothing'] = perplexity_no_smoothing\n",
        "\n",
        "    # 2. Add-k Smoothing and Laplace Smoothing\n",
        "    k_values = [0.5, 3]\n",
        "    laplace_probs, laplace_log_probs = laplace_smoothing(bigram_counts, unigram_counts, vocab_size)\n",
        "    perplexity_laplace = calculate_perplexity(laplace_probs, val_bigrams, laplace_log_probs)\n",
        "    results['Perplexity - Laplace Smoothing'] = perplexity_laplace\n",
        "\n",
        "    for k in k_values:\n",
        "        add_k_probs, add_k_log_probs = add_k_smoothing(bigram_counts, unigram_counts, k, vocab_size)\n",
        "        perplexity_add_k = calculate_perplexity(add_k_probs, val_bigrams, add_k_log_probs)\n",
        "        results[f'Perplexity - Add-k Smoothing (k={k})'] = perplexity_add_k\n",
        "\n",
        "    # 3. Add-k Smoothing and Laplace Smoothing with 'unk' (zero frequency)\n",
        "    train_bigrams_unk = [(train_tokens_unk[i], train_tokens_unk[i + 1]) for i in range(len(train_tokens_unk) - 1)]\n",
        "    bigram_probs_unk, bigram_log_probs_unk, unigram_counts_unk, bigram_counts_unk = calculate_bigram_probabilities(train_tokens_unk, train_bigrams_unk)\n",
        "\n",
        "    laplace_unk_probs, laplace_unk_log_probs = laplace_smoothing(bigram_counts_unk, unigram_counts_unk, len(unigram_counts_unk))\n",
        "    perplexity_laplace_unk = calculate_perplexity(laplace_unk_probs, val_bigrams, laplace_unk_log_probs)\n",
        "    results['Perplexity - Laplace Smoothing (unk)'] = perplexity_laplace_unk\n",
        "\n",
        "    for k in k_values:\n",
        "        add_k_unk_probs, add_k_unk_log_probs = add_k_smoothing(bigram_counts_unk, unigram_counts_unk, k, len(unigram_counts_unk))\n",
        "        perplexity_add_k_unk = calculate_perplexity(add_k_unk_probs, val_bigrams, add_k_unk_log_probs)\n",
        "        results[f'Perplexity - Add-k Smoothing (unk, k={k})'] = perplexity_add_k_unk\n",
        "\n",
        "    # 4. Add-k Smoothing and Laplace Smoothing with rare words as 'unk'\n",
        "    train_bigrams_rare_unk = [(train_tokens_rare_unk[i], train_tokens_rare_unk[i + 1]) for i in range(len(train_tokens_rare_unk) - 1)]\n",
        "    bigram_probs_rare, bigram_log_probs_rare, unigram_counts_rare, bigram_counts_rare = calculate_bigram_probabilities(train_tokens_rare_unk, train_bigrams_rare_unk)\n",
        "\n",
        "    laplace_rare_probs, laplace_rare_log_probs = laplace_smoothing(bigram_counts_rare, unigram_counts_rare, len(unigram_counts_rare))\n",
        "    perplexity_laplace_rare = calculate_perplexity(laplace_rare_probs, val_bigrams, laplace_rare_log_probs)\n",
        "    results['Perplexity - Laplace Smoothing (rare words)'] = perplexity_laplace_rare\n",
        "\n",
        "    for k in k_values:\n",
        "        add_k_rare_probs, add_k_rare_log_probs = add_k_smoothing(bigram_counts_rare, unigram_counts_rare, k, len(unigram_counts_rare))\n",
        "        perplexity_add_k_rare = calculate_perplexity(add_k_rare_probs, val_bigrams, add_k_rare_log_probs)\n",
        "        results[f'Perplexity - Add-k Smoothing (rare words, k={k})'] = perplexity_add_k_rare\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "vZZEOIVoZw-4"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Display Results**"
      ],
      "metadata": {
        "id": "8lE_Zm4XdTPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(results):\n",
        "    print(\"Perplexity Results:\")\n",
        "    for scenario, perplexity in results.items():\n",
        "        print(f\"{scenario}: {perplexity}\")\n",
        "\n",
        "# Run all perplexity calculations\n",
        "results = calculate_all_perplexities()\n",
        "\n",
        "# Display all the results\n",
        "display_results(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3odsAZ5Z0hX",
        "outputId": "107f82de-db87-426b-a64d-46a2a419760d"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity Results:\n",
            "Perplexity - No Smoothing: 9.593040871653818\n",
            "Perplexity - Laplace Smoothing: 60.17091971583495\n",
            "Perplexity - Add-k Smoothing (k=0.5): 43.7558536740361\n",
            "Perplexity - Add-k Smoothing (k=3): 97.7601456201918\n",
            "Perplexity - Laplace Smoothing (unk): 135.37235609080957\n",
            "Perplexity - Add-k Smoothing (unk, k=0.5): 91.95803128753613\n",
            "Perplexity - Add-k Smoothing (unk, k=3): 270.69792197096535\n",
            "Perplexity - Laplace Smoothing (rare words): 88.98504438081802\n",
            "Perplexity - Add-k Smoothing (rare words, k=0.5): 64.32474776593875\n",
            "Perplexity - Add-k Smoothing (rare words, k=3): 162.17594532867278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "LE5qrHzkkF2A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}